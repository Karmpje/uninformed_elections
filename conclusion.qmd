# Conclusion

On the highest level, this project attempted to provide an answer confirming or denying the existence of uninformation in the U.S. 2020 pre-election data. For this sake, uninformation has been defined as any deviation from the rational choice theory, i.e., a set of choices motivated by biases, lack of access to information, or emotions—in short, any case where a decision would have been different if one was better informed on the possible scope of actions. At each stage, an attempt was made to differentiate Respondents based on their declared party affiliation, eventually connecting the obtained results to the outcome of the 2020 Presidential Race.

To test for the robustness of the performed analysis, a specific approach had to be drafted to transcend data obtained from the rather typical question on the Respondent's highest level of attained education. This was the rationale for introducing the Political Science Knowledge metric obtained from the five questions testing one’s cognizance of politics. The move was to ensure that uninformation does not invade the analysis from the beginning onward, as one might lie (presumably introducing a positive shock) when self-reporting the highest level of attained education. Interestingly, the alluvial diagram of respondents’ performance on the Political Knowledge Catch questions hinted at various individuals likely attempting to cheat by correctly answering a tricky question but missing simpler ones.

A few conclusions appeared to be consistent with the generally recognized theory in political science. For example, Respondents with university degrees were more likely to exhibit a higher level of political awareness. Self-reporting turned out to be less reliable than reporting on others, which was the best predictor of calling a presidential race at the country’s level. Similarly, leading questions were shown to have impacted the collected answers; polarization was found to have permeated the reported emotions, and several well-established and commonly used metrics were confirmed to be purely relational.

Nevertheless, while some of these irrational trends could be easily illustrated with the available data, establishing causal relations was rarely possible due to the number of potential explanations driving each result. Moreover, clearly defining the nature of a given instance of uninformation was often impossible, especially if a survey question was not drafted initially to test for the hypothesized difference (e.g., questionnaire splices challenging the impact of leading questions). For this reason, one can only attempt to show a specific phenomenon's existence without explaining the mechanisms operating behind the scenes. This problem was most evident in the polarization heatmaps and economic indicator Cleveland dot plots, where certain conclusions were relatively freely extrapolated from other findings reported in this project or from common sense.

The project could certainly be taken to a higher level if one particular instance of uninformation—say, emotions, and not rationality, driving choices—was to be investigated in depth from all the possible perspectives. A somewhat simpler direction for future research would be to consider the U.S. 2020 post-election data to see if the described trends also hold there and available data for any previous presidential election year (for example, 2016 or 2012). Such a panel approach could track the evolution of trends and possibly rule out the cases of using the surveys to express dissatisfaction fueled by a mismatch between one’s political affiliation and the political affiliation of the incumbent president—where these possible combinations are guaranteed to exist in a comparison between 2016 and 2020 data. A valuable future input could also be achieved by designing methods, tools, or algorithms that could impartially tackle and tame relativity in data on uninformation, i.e., a difference in interpretation based on the accepted stand.