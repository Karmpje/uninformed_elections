[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Uninformation in the U.S. Elections Data",
    "section": "",
    "text": "1 Introduction\nThe concept of polarization is a long-standing and well-known phenomenon in political sciences, especially relevant to democratic government systems. In plain English, polarization refers to the division of society into distinct groups based on differing agendas, beliefs, opinions, goals, needs, and priorities, which are expressed and debated as part of the democratic process. As a result, these divisions often shape the direction of legislation and policy drafting as opposing factions attempt to advance their values, ethics, and moral frameworks through executive governance.\nThe most common examples of polarization occur along ideological divides, such as the left and right political spectrum (e.g., liberalism and conservatism), religion (secularism and theocracy), economics (communism and capitalism), and social policy (libertarianism and authoritarianism). In countries with a two-party system (an example of which is the United States), the polarizing divide predominantly matches the tensions of partisan identities.\nWhile polarization occurs in any democracy, an interesting detail unique to the United States is the persistence and stability of these societal and political divides. In other words, voters, states, and politicians often adhere strictly to the party lines at any given decision time. Deviations from the stable voting trajectory patterns are rare but highly consequential, usually reflecting major societal dissatisfaction. Moreover, these abnormalities are immensely powerful in turning the scales, as it is safe to assume that each party has a relatively equal electorate population. Thus, while statistically almost improbable, U.S. history is abundant in examples of a single vote tipping the election balance (see, for example, https://middletonma.gov/303/The-Power-of-One-Vote).\nThis project seeks to examine cases where electoral outcomes diverge from established partisan norms or reported public opinion. Identifying these nonconformities could shed light on the presence and impact of structural misinformation or disinformation within American society. Per rational choice theory (i.e., justified decision-making), any such divergence shall be deemed a priori irrational, for it does not reflect true and honest communication of voter preferences. Thus, understanding these gaps could provide insights into the mechanisms driving such irregularities and offer a deeper understanding of voter priorities extending beyond the traditional party lines.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data",
    "section": "",
    "text": "2.1 Description\nEvery four years, matching the pattern of presidential elections in the United States, two studies are conducted in the pre-election and post-election periods, with the November Election Day holiday marking the turning point between the two. Each study interviews respondents online, by phone, and through live video software before the voting day. Then, it attempts to re-interview as many of the same respondents as possible after the voting day. Phone and video interviews are facilitated by trained interviewers who belong to ANES.\nRaw datasets (with the exclusion of sensitive variables that could identify particular respondents) are available for free in multiple formats, including the .csv one, on the ANES website. These are files with data frames characterized by huge dimensionality (but relatively small file size), where the number of rows (representing respondents) oscillates between ~6-8k, and the number of variables is around ~1.8k. Thus, importing the data is relatively simple as it relies on downloading and reading the respective .csv file. For the 2020 Time Series Study election data (analyzed in the missing values section), there are 8280 respondents and 1771 variables. Some variables are categorical (e.g., yes/no questions), and some are numerical (e.g., age). Sampling weights allowing for correct scalability of results to the entire U.S. voting population are also given.\nVariables reflect the two study periods; pre-election variables start with the code “V201,” and post-election variables start with the code “V202.” The exact code-to-question mapping, as well as a thorough description of the methodology covering possible values per question, is accessible in the accompanying “User Guide and Codebook” published alongside each dataset on the ANES website. It is said to be one of the largest datasets on voting, public opinions, and voter tendencies in the United States.\nIn these datasets, missing values are denoted with “-9” or “-8” codes. The former captures skipped questions and answers that the respondent declined to give; it is also the most prevalent. The latter is less frequent as it encodes the explicit answer “I don’t know,” which might have been given after the respondent was prompted with a specific question. Nevertheless, while missing values conceal some data trends, the sheer number of available variables proves the most challenging to deal with, as any analysis requires a careful variable selection or dimensionality reduction to be performed first. Otherwise, irrelevant factors (noise) might be erroneously considered.\nFull Citation (again, specifically for the 2020 data): American National Election Studies. 2021. ANES 2020 Time Series Study Full Release [dataset and documentation]. February 10, 2022 version. www.electionstudies.org",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2  Data",
    "section": "2.2 Missing value analysis",
    "text": "2.2 Missing value analysis\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(forcats)\n\n\n\n\nCode\n# loading the dataset\nanes_data &lt;- read_csv(\"data/raw/anes_timeseries_2020_csv_20220210.csv\", show_col_types = FALSE)\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nCode\n# selecting variables of interest\npre_columns &lt;- grep(\"^V201(00[1-9]|0[1-9][0-9]|[1-5][0-9][0-9]|6[0-5][0-8])$\", colnames(anes_data), value = TRUE)\npre_election_data &lt;- anes_data[, pre_columns]\npost_columns &lt;- grep(\"^V202(00[1-9]|0[1-9][0-9]|[1-6][0-4][0-5])$\", colnames(anes_data), value = TRUE)\npost_election_data &lt;- anes_data[, post_columns]\n\n# filtering for NAs encoded as -9\npre_columns_with_negative_nine &lt;- sapply(pre_election_data, function(x) any(x == -9, na.rm = TRUE))\npre_missing_data &lt;- pre_election_data[, pre_columns_with_negative_nine]\npost_columns_with_negative_nine &lt;- sapply(post_election_data, function(x) any(x == -9, na.rm = TRUE))\npost_missing_data &lt;- post_election_data[, post_columns_with_negative_nine]\n\n# calculating the proportions of missing data and setting the threshold\nthreshold &lt;- 0.02\nproportion_negative_nine_pre &lt;- sapply(pre_missing_data, function(x) mean(x == -9))\npre_sorted_proportions &lt;- sort(proportion_negative_nine_pre, decreasing = TRUE)\npre_columns_to_include &lt;- names(pre_sorted_proportions[pre_sorted_proportions &gt; threshold])\n\nproportion_negative_nine_post &lt;- sapply(post_missing_data, function(x) mean(x == -9))\npost_sorted_proportions &lt;- sort(proportion_negative_nine_post, decreasing = TRUE)\npost_columns_to_include &lt;- names(post_sorted_proportions[post_sorted_proportions &gt; threshold])\n\n# helper to map the question codes to full questions\nquestion_mapping &lt;- c(\n  \"V201151\" = \"Rating: Joe Biden?\",\n  \"V201152\" = \"Rating: Donald Trump?\",\n  \"V201153\" = \"Rating: Kamala Harris?\",\n  \"V201154\" = \"Rating: Mike Pence?\",\n  \"V201203\" = \"Scale Liberal-Conservative:\\nDonald Trump\",\n  \"V201204\" = \"Scale Liberal-Conservative:\\nDemocratic House Candidate\",\n  \"V201205\" = \"Scale Liberal-Conservative:\\nRepublican House Candidate\",\n  \"V201338\" = \"Stand on Abortion:\\nDemocratic Candidates\",\n  \"V201339\" = \"Stand on Abortion:\\nRepublican Candidates\",\n  \"V201409\" = \"Transgender Policy\",\n  \"V201529\" = \"Responder's Employment\",\n  \"V201576\" = \"How long has Responder lived\\nin the present community?\",\n  \"V201606\" = \"Money Invested in Stock Markets\",\n  \"V201628\" = \"Number of Guns Owned\",\n  \"V201642\" = \"Political Knowledge\\nCatch Question (Hard)\",\n  \"V201644\" = \"Political Knowledge\\nCatch Question (Easy)\"\n)\n\n\n\n\nCode\n# missing data for pre-elections: proportions\nmissing_df &lt;- data.frame(variable = names(proportion_negative_nine_pre), \n                          missing_proportion = proportion_negative_nine_pre)\nmissing_df_filtered &lt;- missing_df[missing_df$missing_proportion &gt; threshold, ]\nmissing_df_filtered$variable_label &lt;- question_mapping[missing_df_filtered$variable]\n\nggplot(missing_df_filtered, aes(y = reorder(variable_label, missing_proportion), x = missing_proportion)) +\n  geom_bar(stat = \"identity\", fill = \"blue\") +\n  labs(title = \"Proportion of Missing Values in Pre-Election 2020 Data (Threshold &gt; 2%)\",\n       x = \"Missing Proportion\", y = \"Questions\") +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nThe large amount of data makes it necessary to take a very selective approach when looking at missing values. For the purpose of this particular analysis, an emphasis is put on the questions in the pre-election data where the proportion of missing values is above 2%. This threshold was chosen somewhat arbitrarily, but it helps separate the clear outliers from the many questions with missing values within the 1% range.\nNot surprisingly, the political knowledge hard catch question has the highest proportion of missing values, with more than 10% of respondents unable to answer it. While this stands out as an extreme case, the next two highest proportions (between 7.5% and 10%) are more interesting. These come from questions asking respondents to place their local Democratic and Republican House candidates on the liberal-conservative scale. Since most respondents are probably familiar with the scale, it is unlikely that a lack of understanding caused the high missing rates here. Instead, it might suggest that many respondents simply do not know their local party candidates, which could be even more common if the candidate is from the opposite party.\nThe fourth-highest proportion of missing values comes from the question about the number of guns owned, which is likely due to the sensitive nature of this topic. People may be hesitant to share such personal and identifying information.\nThe fifth-highest missing value proportion comes from the question about rating Kamala Harris, who was running for vice president at the time. In 2020, Harris seemed to have a much higher rate of unrecognizability compared to Mike Pence, Donald Trump’s running mate. This might be because Harris was newer to the national political stage, while Pence was already a well-known figure. Possibly, this could have started a domino-effect extending into the 2024 presidential elections.\n\n\nCode\n# missing data for pre-elections: heatmap\nset.seed(123)\nsampled_data &lt;- pre_missing_data[sample(nrow(pre_missing_data), 100), ]\n\ntidy_sampled_data &lt;- sampled_data |&gt;\n  rownames_to_column(\"id\") |&gt;\n  pivot_longer(cols = all_of(pre_columns_to_include)) |&gt;\n  mutate(missing = ifelse(value == -9, \"Yes\", \"No\"))\n\ntidy_sampled_data$variable_label &lt;- question_mapping[tidy_sampled_data$name]\n\nquestion_order &lt;- c(\"Rating: Joe Biden?\",\"Rating: Donald Trump?\",\"Rating: Kamala Harris?\",\"Rating: Mike Pence?\",\n                    \"Scale Liberal-Conservative:\\nDonald Trump\",\"Scale Liberal-Conservative:\\nDemocratic House Candidate\",\n                    \"Scale Liberal-Conservative:\\nRepublican House Candidate\",\"Stand on Abortion:\\nDemocratic Candidates\",\n                    \"Stand on Abortion:\\nRepublican Candidates\",\"Transgender Policy\",\"Responder's Employment\",\n                    \"How long has Responder lived\\nin the present community?\",\"Money Invested in Stock Markets\",\n                    \"Number of Guns Owned\",\"Political Knowledge\\nCatch Question (Hard)\",\"Political Knowledge\\nCatch Question (Easy)\"\n)\n\ntidy_sampled_data$variable_label &lt;- factor(\n  tidy_sampled_data$variable_label,\n  levels = question_order\n)\n\nggplot(tidy_sampled_data, aes(x = variable_label, y = id, fill = missing)) +\n  geom_tile(color = \"white\") +\n  scale_fill_viridis_d() +\n  labs(title = paste(\"Pre-Elections 2020 Data with Missing Values (100 Random Samples)\"),\n       x = \"Variables\", y = \"Sample Number\", fill = \"Missing\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCreating a meaningful heatmap to show missing value patterns for the same individuals across different questions is tricky because of the large amount of data. To make it more manageable, a (pseudo)random sample of 100 individuals is used, along with the same subset of questions as before–those with more than 2% missing values. However, this method makes it hard to tell if a sequence of missing values happens because someone decided to stop participating in the study. If that is the case, the missing values (shown in yellow) would extend to the far-right side of the heatmap, but there is no way to be sure with the chosen approach.\nSome patterns seen earlier show up here, too. For example, respondents who skipped the political knowledge hard catch question also tended to skip the questions about placing their local Democratic and Republican House candidates on the liberal-conservative scale.\nMoreover, there seem to be two common patterns when it comes to missing answers. In some cases, the respondent struggles with just one question and skips it rather than selecting “I don’t know.” In other cases, the person skips several questions, which could happen because they lack the knowledge needed to answer these or, perhaps, because they want to finish the survey quickly. While neither hypothesis can be confirmed based on this small sample, these patterns are worth exploring further. For instance, if someone skips just one question about recognizing a candidate, it might mean they truly do not know who that candidate is. On the other hand, skipping multiple questions, like rating or demographic questions, could be a way to hide the respondent’s preferences or identity. Both possibilities are worth considering, and more detailed visualizations could help confirm these trends.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "3  Results",
    "section": "",
    "text": "3.1 What is a better prediction metric: the highest level of attained education or the level Political Science Knowledge?\nCode\n# reading data\ndf &lt;- read_csv(\"data/raw/anes_timeseries_2020_csv_20220210.csv\", show_col_types = FALSE)\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nCode\nalluvials &lt;- df |&gt; dplyr::select(V200001,V201642,V201644,\n                                 V201645,V201646,V201647,V201075x)\n\n# data cleaning and preparation\nalluvials &lt;- alluvials |&gt; mutate(V201642 = ifelse(V201642 == 1896, 'Correct',\n                                                  ifelse(V201642 == -9, 'Incorrect',\n                                                         ifelse(V201642 == -5, 'dropped', 'Incorrect'))))\n\nalluvials &lt;- alluvials |&gt; mutate(V201644 = ifelse(V201644 == 6, 'Correct',\n                                                  ifelse(V201644 == -9, 'Incorrect',\n                                                         ifelse(V201644 == -5, 'dropped', 'Incorrect'))))\n\nalluvials &lt;- alluvials |&gt; mutate(V201645 = ifelse(V201645 == 1, 'Correct',\n                                                  ifelse(V201645 == -9, 'Incorrect',\n                                                         ifelse(V201645 == -5, 'dropped', 'Incorrect'))))\n\nalluvials &lt;- alluvials |&gt; mutate(V201646 = ifelse(V201646 == 1, 'Correct',\n                                                  ifelse(V201646 == -9, 'Incorrect',\n                                                         ifelse(V201646 == -5, 'dropped', 'Incorrect'))))\n\nalluvials &lt;- alluvials |&gt; mutate(V201647 = ifelse(V201647 == 2, 'Correct',\n                                                  ifelse(V201647 == -9, 'Incorrect',\n                                                         ifelse(V201647 == -5, 'dropped', 'Incorrect'))))\n\nalluvials &lt;- alluvials |&gt; mutate(V201075x = ifelse(V201075x %in% c(10, 20, 30), 'Democrat',\n                                                   ifelse(V201075x %in% c(11, 21, 31), 'Republican',\n                                                          ifelse(V201075x %in% c(12, 22, 32), 'Independent', 'Inapplicable'))))\n\nalluvials &lt;- alluvials |&gt; rowwise() |&gt; \n  mutate(\n    PolSciKnow = case_when(\n      sum(as.numeric(c_across(V201642:V201647) == \"Correct\")) &gt;= 4 ~ \"High\",\n      sum(as.numeric(c_across(V201642:V201647) == \"Correct\")) &lt; 3 ~ \"Low\",\n      TRUE ~ \"Medium\"\n    )\n  ) |&gt; \n  ungroup()\n\nalluvials &lt;- alluvials |&gt; mutate(Cheated = ifelse(V201642 == \"Correct\" & (PolSciKnow == \"Low\" | PolSciKnow == \"Medium\"), \"Yes\", \"No\"))\n\nalluvials_plot &lt;- alluvials |&gt;\n  group_by(V201642,V201644,V201645,V201646,V201647,V201075x,PolSciKnow) |&gt;\n  summarize(Freq = n(), .groups = 'keep')\n\n\n# reducing dimensionality further\nalluvials_plot &lt;- alluvials_plot |&gt; filter(V201075x != \"Inapplicable\")\nalluvials_plot &lt;- alluvials_plot |&gt; filter(V201075x != \"Independent\")\nalluvials_plot &lt;- alluvials_plot |&gt; filter(V201642 != 'dropped')\nalluvials_plot &lt;- alluvials_plot |&gt; filter(V201644 != 'dropped')\nalluvials_plot &lt;- alluvials_plot |&gt; filter(V201645 != 'dropped')\nalluvials_plot &lt;- alluvials_plot |&gt; filter(V201646 != 'dropped')\nalluvials_plot &lt;- alluvials_plot |&gt; filter(V201647 != 'dropped')\n\nalluvials_plot &lt;- alluvials_plot |&gt; rowwise() |&gt;\n  mutate(Proportion = Freq/sum(alluvials_plot$Freq))\n\nalluvials_plot$PolSciKnow &lt;- factor(alluvials_plot$PolSciKnow, levels = c(\"High\", \"Medium\", \"Low\"))\nCode\nggplot(alluvials_plot, aes(axis1=V201647, axis2 = V201646, axis3 = V201644, axis4 = V201645, axis5 = V201642, axis6 = PolSciKnow, y = Proportion)) +\n  geom_flow(aes(fill = V201075x), color = 'black') +\n  geom_stratum() + \n  geom_text(stat = 'stratum', aes(label = paste(after_stat(stratum))), size = 3) +\n  scale_x_discrete(limits = c(\"Which party has the most\\nmembers in Senate right now?\",\n                              \"Which party has the most\\nmembers in Congress right now?\",\n                              \"How many years are there\\nin one full term of a US Senator?\",\n                              \"On which program does the\\nFederal Government spend the least?\", \n                              \"When did the Supreme Court\\ndecide on Geer v. Connecticut?\",\n                              \"Level of Political Science Knowledge\"), name = \"Results\") +\n    labs(title = \"Respondents' Performance on the Political Knowledge Catch Questions\", fill = \"Party Affiliation\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  scale_fill_manual(values = c('Democrat' = \"blue\", 'Republican' = \"red\"))\n\n\nWarning in to_lodes_form(data = data, axes = axis_ind, discern =\nparams$discern): Some strata appear at multiple axes.\nWarning in to_lodes_form(data = data, axes = axis_ind, discern =\nparams$discern): Some strata appear at multiple axes.\nWarning in to_lodes_form(data = data, axes = axis_ind, discern =\nparams$discern): Some strata appear at multiple axes.\nTo start tackling the first research question motivating this project with its search for cases of uninformation in the U.S. elections data, consider this alluvial chart summarizing the respondents’ performance on the five political knowledge catch questions included in the questionnaire. Recall that the “hard” question (corresponding to the “When did the Supreme Court decide on Geer v. Connecticut?”) was the item with the highest proportion of missing value; in other words, respondents were most likely not to answer it. This phenomenon is further emphasized here by the vast majority of respondents, among those who did attempt this question, providing an incorrect answer. Nevertheless, such a behavior was expected as one needs to exhibit legal knowledge prowess to know the correct answer!\nAlluvia (the horizontal ‘lines’) were colored in a way that tests for the association between the performance on this test and the respondent’s party affiliation. The ordering of questions here does not match the ordering of these questions in the original questionnaire; for the sake of a simplified analysis, questions were organized here by their perceived difficulty according to the recorded answers. Finally, based on the number of correctly answered questions, the respondents fell into one of the categories–High (4 or more correct answers), Medium (exactly 3), or Low (2 or less)–representing their overall level of Political Science Knowledge. These boundaries were chosen in a way that condones missing the very hard Supreme Court question and still making it to the “High” category, while necessitating getting all the basics right (i.e., majorities in the House and Senate + length of term of US Senators) to score at the “Medium” level.\nAs the questionnaire was filled by more respondents affiliated with the Democratic party, it is unsurprising that the proportion of Democrats falling into any category is slightly higher than the proportion of Republicans achieving the same result. Yet, even when one accounts for this small difference, the biggest disproportion seems to happen between the third and fourth question at the level of correct answers; here the proportion of Republicans answering both correctly seems to be much smaller than the proportion of Democrats. This might hint at the higher level of uninformation among Republicans regarding the the spendings of the Federal Government–an issue which usually ignites the public debate around the elections.\nAnother problem seems to be revealed by the general performance on the third question, with almost 60% of respondents getting it wrong. In other words, 60% of respondents do not know the length of US senators’ terms even though they are regularly asked to elect these officials into the office.\nOn a separate note, this chart also exhibits the first instances of cheating on the questionnaire. This is illustrated with the small alluvia that manage to get the Supreme Court question correct, while not making it exclusively to the High category of Political Science Knowledge. It is extremely unlikely that somebody could correctly provide a year (or guess it) for this judicial case while failing to answer perceivably simpler questions. This could have happened if the person of concern has somehow cheated, e.g., by looking the answer up online.\nCode\n# data cleaning for Plot 2\n\neducation_1 &lt;- alluvials |&gt; dplyr::select(V200001, V201075x, PolSciKnow, Cheated)\neducation_2 &lt;- df |&gt; dplyr::select(V200001, V201001, V201600, V201005, V201006, V201007a, V201511x)\neducation &lt;- full_join(education_1, education_2, by = \"V200001\")\n\n\neducation &lt;- education |&gt; mutate(V201001 = ifelse(V201001 == 1, \"English\", \"Spanish\"))\n\neducation &lt;- education |&gt; mutate(V201600 = ifelse(V201600 == 1, \"M\",\n                                                  ifelse(V201600 == 2, \"F\", 0)))\n\neducation &lt;- education |&gt; mutate(V201005 = ifelse(V201005 == 1, \"Always\",\n                                                  ifelse(V201005 == 2, \"Often\",\n                                                         ifelse(V201005 == 3, \"Sometimes\",\n                                                                ifelse(V201005 == 4, \"Rarely\",\n                                                                       ifelse(V201005 == 5, \"Never\", 0))))))\n\neducation &lt;- education |&gt; mutate(V201006 = ifelse(V201006 == 1, \"Very much interested\",\n                                                  ifelse(V201006 == 2, \"Somewhat interested\",\n                                                         ifelse(V201006 == 3, \"Not much interested\", 0))))\n\neducation &lt;- education |&gt; mutate(V201007a = ifelse(V201007a == 1, \n                                                   \"Democrat first, Republican second\",\n                                                   \"Republican first, Democrat second\"))\n\neducation &lt;- education |&gt; mutate(V201511x = ifelse(V201511x == 1, \"Grade School\",\n                                                   ifelse(V201511x == 2, \"High School\",\n                                                          ifelse(V201511x == 3, \"Some College\",\n                                                                 ifelse(V201511x == 4, \"Bachelor's Degree\",\n                                                                        ifelse(V201511x == 5, \"Graduate degree\", 0))))))\n\n\neducation &lt;- education |&gt; filter(V201075x != \"Inapplicable\")\neducation &lt;- education |&gt; filter(V201075x != \"Independent\")\neducation &lt;- education |&gt; filter(V201600 != 0)\neducation &lt;- education |&gt; filter(V201005 != 0)\neducation &lt;- education |&gt; filter(V201006 != 0)\neducation &lt;- education |&gt; filter(V201511x != 0)\n\neducation$V201075x &lt;- factor(education$V201075x, levels=c(\"Democrat\",\"Republican\"))\neducation$PolSciKnow &lt;- factor(education$PolSciKnow, levels=c(\"High\",\"Medium\",\"Low\"))\neducation$Cheated &lt;- factor(education$Cheated, levels=c(\"Yes\",\"No\"))\neducation$V201001 &lt;- factor(education$V201001, levels=c(\"English\",\"Spanish\"))\neducation$V201600 &lt;- factor(education$V201600, levels=c(\"F\",\"M\"))\neducation$V201005 &lt;- factor(education$V201005, levels=c(\"Always\",\"Often\",\"Sometimes\",\"Rarely\",\"Never\"))\neducation$V201006 &lt;- factor(education$V201006, levels=c(\"Very much interested\",\n                                                        \"Somewhat interested\",\"Not much interested\"))\neducation$V201007a &lt;- factor(education$V201007a, levels=c(\"Democrat first, Republican second\",\n                                                          \"Republican first, Democrat second\"))\neducation$V201511x &lt;- factor(education$V201511x, levels=c(\"Grade School\",\"High School\",\"Some College\",\n                                                          \"Bachelor's Degree\",\"Graduate degree\"))\n\nnames(education)[2] &lt;- \"Party\"\nnames(education)[3] &lt;- \"PolSci_Knowledge\"\nnames(education)[6] &lt;- \"Sex\"\nnames(education)[10] &lt;- \"Education\"\nCode\nmosaic(~ Party + Education + Sex + PolSci_Knowledge,\n       data = education,\n       direction =c(\"h\", \"v\", \"v\", \"h\"),\n       highlighting = \"PolSci_Knowledge\",\n       main = \"Association between the level of Political Science Knowledge and Respondent's Education\")\nTo relate the level of Political Science Knowledge to Respondent’s Education, consider this Mosaic Plot which visually tests for the association (correlation) between these two variables. If they were unrelated, the horizontal cuts splitting the bars would happen more less at the same level throughout the entire plot. Clearly, this is not the case.\nThe immediate conclusion is that the level of Political Science Knowledge, whose computation was illustrated in the previous chart, is positively correlated with Respondent’s education; the higher one’s highest attained educational level is, the higher probability that this person would be classified as having a High level of Political Science Knowledge. Moreover, this approach fixes the problem of unequal proportions of Democrats and Republicans answering the questionnaire that blemished the previous plot. The height of the lightest gray bars corresponding to the High Political Science Knowledge is almost always bigger for Democrats than for Republicans at any given educational level. The structure of the plot also exhibits that while almost half of Democrats attempting the questionnaire holds at least a Bachelor’s degree, the proportion of Republicans with the same qualifications is much smaller.\nGenerally speaking, there is also such a lightest gray bar difference between males and females with similar level of education. Interestingly, the proportion of better educated females with High Political Science Knowledge is almost always the same as the proportion of worse educated males with High Political Science Knowledge up until the completion of undergraduate studies, where one extra attained level of education does not allow females to catch up with males in terms of their Political Science Knowledge. This is represented by the big jumps between F and M categories belonging to different Education levels (considered exactly in the order presented in this sentence) that start becoming prevalent from Bachelor’s degree onward. Unfortunately, such a trend hints at a sex-driven uninformation bias.\nMoving forward, the results illustrated in this Mosaic Plot suggest that it might be worth focusing on the level of Respondents’ Political Science Knowledge rather than just their highest level of attained education to uncover more data trends. The proportion of Respondents with university degrees not being able to correctly answer basic questions on the US government is simply too high to believe that a higher level of education might proof a given Respondent from uninformation. In other words, and quite sadly, there is no one-to-one mapping between a university degree and at least a basic (medium or high) level of knowledge on the US politics.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#who-is-the-best-informed-then-if-not-the-university-educated-ones",
    "href": "results.html#who-is-the-best-informed-then-if-not-the-university-educated-ones",
    "title": "3  Results",
    "section": "3.2 Who is the best informed then, if not the university-educated ones?",
    "text": "3.2 Who is the best informed then, if not the university-educated ones?\n\n\nCode\nwinners_1 &lt;- alluvials |&gt; dplyr::select(V200001, V201075x, PolSciKnow, Cheated)\nwinners_2 &lt;- df |&gt; dplyr::select(V200001, V201217, V201219, V201237, V201014b)\nwinners &lt;- full_join(winners_1, winners_2, by = \"V200001\")\n\nwinners &lt;- winners |&gt; filter(V201014b != -9 & V201014b != -8 & V201014b != -1 & V201014b != 86)\n\nstate_labels &lt;- c(\n  \"Alabama\" = 1, \"Alaska\" = 2, \"Arizona\" = 4, \"Arkansas\" = 5, \"California\" = 6,\n  \"Colorado\" = 8, \"Connecticut\" = 9, \"Delaware\" = 10, \"DC\" = 11,\n  \"Florida\" = 12, \"Georgia\" = 13, \"Hawaii\" = 15, \"Idaho\" = 16, \"Illinois\" = 17,\n  \"Indiana\" = 18, \"Iowa\" = 19, \"Kansas\" = 20, \"Kentucky\" = 21, \"Louisiana\" = 22,\n  \"Maine\" = 23, \"Maryland\" = 24, \"Massachusetts\" = 25, \"Michigan\" = 26,\n  \"Minnesota\" = 27, \"Mississippi\" = 28, \"Missouri\" = 29, \"Montana\" = 30,\n  \"Nebraska\" = 31, \"Nevada\" = 32, \"New Hampshire\" = 33, \"New Jersey\" = 34,\n  \"New Mexico\" = 35, \"New York\" = 36, \"North Carolina\" = 37, \"North Dakota\" = 38,\n  \"Ohio\" = 39, \"Oklahoma\" = 40, \"Oregon\" = 41, \"Pennsylvania\" = 42,\n  \"Rhode Island\" = 44, \"South Carolina\" = 45, \"South Dakota\" = 46,\n  \"Tennessee\" = 47, \"Texas\" = 48, \"Utah\" = 49, \"Vermont\" = 50,\n  \"Virginia\" = 51, \"Washington\" = 53, \"West Virginia\" = 54,\n  \"Wisconsin\" = 55, \"Wyoming\" = 56\n)\nstate_labels &lt;- setNames(names(state_labels), state_labels)\nlabel_states &lt;- function(state_id) {\n  state_labels[as.character(state_id)]\n}\n\nwinners &lt;- winners |&gt;\n  mutate(V201014b = label_states(V201014b))\n\nwinners &lt;- winners |&gt; filter(V201217 != -9 & V201217  != -8 & V201217  != 5)\nwinners &lt;- winners |&gt; mutate(V201217 = ifelse(V201217 == 1, \"Joe Biden\", \"Donald Trump\"))\nwinners$V201217 &lt;- factor(winners$V201217, levels=c(\"Donald Trump\",\"Joe Biden\"))\n\nwinners &lt;- winners |&gt; filter(V201219 != -9 & V201219 != -8 & V201219 != 5)\nwinners &lt;- winners |&gt; mutate(V201219 = ifelse(V201219 == 1, \"Joe Biden\", \"Donald Trump\"))\nwinners$V201219 &lt;- factor(winners$V201219, levels=c(\"Donald Trump\",\"Joe Biden\"))\n\nwinners &lt;- winners |&gt; filter(V201237 != -9 & V201237 != -8)\nwinners &lt;- winners |&gt; mutate(V201237 = ifelse(V201237 == 1, \"Always\", \n                                              ifelse(V201237 == 2, \"Usually\",\n                                                     ifelse(V201237 == 3, \"Sometimes\",\n                                                            ifelse(V201237 == 4, \"Rarely\", \"Never\")))))\nwinners$V201237 &lt;- factor(winners$V201237, levels=c(\"Always\",\"Usually\", \"Sometimes\", \"Rarely\", \"Never\"))\n\nwinners &lt;- winners |&gt; filter(V201075x == \"Democrat\" | V201075x == \"Republican\")\nwinners &lt;- winners |&gt; mutate(V201075x = ifelse(V201075x == \"Democrat\", \"Joe Biden\", \"Donald Trump\"))\n\nstate_winners_S &lt;- winners |&gt; group_by(V201014b) |&gt;\n  summarize(Winner = names(which.max(table(V201217)))) |&gt; ungroup()\n\nstate_winners_O &lt;- winners |&gt; group_by(V201014b) |&gt;\n  summarize(Winner = names(which.max(table(V201219)))) |&gt; ungroup()\n\nstate_winners_V &lt;- winners |&gt; group_by(V201014b) |&gt;\n  summarize(Winner = names(which.max(table(V201075x)))) |&gt; ungroup()\n\nstate_winners_S &lt;- state_winners_S |&gt;\n  mutate(state = tolower(V201014b))\n\nstate_winners_O &lt;- state_winners_O |&gt;\n  mutate(state = tolower(V201014b))\n\nstate_winners_V &lt;- state_winners_V |&gt;\n  mutate(state = tolower(V201014b))\n\nstate_winners_A &lt;- state_winners_O |&gt;\n  mutate(Winner = c(\"Donald Trump\",\"Donald Trump\",\"Joe Biden\",\"Donald Trump\",\"Joe Biden\",\n                    \"Joe Biden\",\"Joe Biden\",\"Joe Biden\",\"Joe Biden\",\"Donald Trump\",\n                    \"Joe Biden\",\"Joe Biden\",\"Donald Trump\",\"Joe Biden\",\"Donald Trump\",\n                    \"Donald Trump\",\"Donald Trump\",\"Donald Trump\",\"Donald Trump\",\"Joe Biden\",\n                    \"Joe Biden\",\"Joe Biden\",\"Joe Biden\",\"Joe Biden\",\"Donald Trump\",\n                    \"Donald Trump\",\"Donald Trump\",\"Donald Trump\",\"Joe Biden\",\"Joe Biden\",\n                    \"Joe Biden\",\"Joe Biden\",\"Joe Biden\",\"Donald Trump\",\"Donald Trump\",\n                    \"Donald Trump\",\"Donald Trump\",\"Joe Biden\",\"Joe Biden\",\"Joe Biden\",\n                    \"Donald Trump\",\"Donald Trump\",\"Donald Trump\",\"Donald Trump\",\"Donald Trump\",\n                    \"Joe Biden\",\"Joe Biden\",\"Joe Biden\",\"Donald Trump\",\"Joe Biden\",\n                    \"Donald Trump\"))\n\n\n\n\nCode\nS &lt;- plot_usmap(data = state_winners_S, values = \"Winner\") +\n  scale_fill_manual(\n    values = c(\"Donald Trump\" = \"red\", \"Joe Biden\" = \"blue\"),  # Set fill colors\n    name = \"Winner\"\n  ) +\n  labs(\n    title = \"Aggregated Individual Opinions\",\n  ) +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"right\")\n\nO &lt;- plot_usmap(data = state_winners_O, values = \"Winner\") +\n  scale_fill_manual(\n    values = c(\"Donald Trump\" = \"red\", \"Joe Biden\" = \"blue\"),  # Set fill colors\n    name = \"Winner\"\n  ) +\n  labs(\n    title = \"Aggregated Individual Opinions\\non the State Winners\",\n  ) +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"right\")\n\nV &lt;- plot_usmap(data = state_winners_V, values = \"Winner\") +\n  scale_fill_manual(\n    values = c(\"Donald Trump\" = \"red\", \"Joe Biden\" = \"blue\"),  # Set fill colors\n    name = \"Winner\"\n  ) +\n  labs(\n    title = \"Aggregated Vote Counts in the Sample\",\n  ) +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"right\")\n\nA &lt;- plot_usmap(data = state_winners_A, values = \"Winner\") +\n  scale_fill_manual(\n    values = c(\"Donald Trump\" = \"red\", \"Joe Biden\" = \"blue\"),  # Set fill colors\n    name = \"Winner\"\n  ) +\n  labs(\n    title = \"Official Results\",\n  ) +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"right\")\n\n(S + V) / (O + A) + plot_layout(guides = \"collect\") +\n  plot_annotation(title = \"Election Winner per...\") & \n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nThese four maps call the 2020 Presidential Election race based on:\n\nAggregated Individual Opinions, i.e., the winner is called based on what the majority of Respondents think on who the winner will be on the national level.\nAggregated Vote Counts in the Sample, i.e., the winner is called based on the most prevalent political affiliation of Respondents per state.\nAggregated Individual Opinions on the State Winners, i.e., the winner is called based on what the majority of Respondents think on who the winner will on the state level.\nOfficial Results, i.e., the actual results of the 2020 Presidential Election race, which were NOT available at the time when Respondents attempted this survey.\n\nNow, if these maps were to be ordered in terms of predictive power, the least powerful would be just choosing the winner based on the political affiliation of the majority of Respondents attempting the questionnaire in a given state, followed by the aggregated opinions on the winner at the national level, and topped with the aggregated opinions on the winner at the state level. This ordering can be constructed based on the number of states that are called incorrectly in relation to the actual winner depicted on the bottom-right map. Somehow, the survey was answered predominantly by Democrats in the Mid-West region, which tipped the scales in favor of Joe Biden in states where it should have not (top-right map), while some of these did not believe in Joe Biden’s win at the country level (top-left map). Nevertheless, when asked about who the winner will be locally, the problem of Mid-West is resolved, Rhode Island turns blue while Florida and Wyoming turn red, and only Arizona and Georgia–the two closest races–are still incorrectly called.\nConclusion: trust your gut feelings about your neighbors. You might be the most informed and unbiased on who those around you might be like, which, if aggregated, becomes extremely powerful in terms of predictive power.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#what-about-the-strength-of-constitution-of-respondents-decisions-or-how-influenceable-are-the-respondents",
    "href": "results.html#what-about-the-strength-of-constitution-of-respondents-decisions-or-how-influenceable-are-the-respondents",
    "title": "3  Results",
    "section": "3.3 What about the strength of constitution … of Respondents’ decisions? Or, how influenceable are the Respondents?",
    "text": "3.3 What about the strength of constitution … of Respondents’ decisions? Or, how influenceable are the Respondents?\n\n\nCode\nvoting_cd_1 &lt;- alluvials |&gt; dplyr::select(V200001, V201075x, PolSciKnow)\nvoting_cd_2 &lt;- df |&gt; dplyr::select(V200001, V201221, V201222, V201225x)\nvoting_cd &lt;- full_join(voting_cd_1, voting_cd_2, by = \"V200001\")\n\nvoting_cd &lt;- voting_cd |&gt; filter(V201221 != -9)\nvoting_cd &lt;- voting_cd |&gt; mutate(V201221 = ifelse(V201221 == 1, \"Mainly a duty\",\n                                                  ifelse(V201221 == 2, \"Mainly a choice\",\n                                                         ifelse(V201221 == 3, \"Neither a duty nor a choice\", 0))))\n\nvoting_cd &lt;- voting_cd |&gt; filter(V201222 != -9)\nvoting_cd &lt;- voting_cd |&gt; mutate(V201222 = ifelse(V201222 == 1, \"Mainly a choice\",\n                                                  ifelse(V201222 == 2, \"Mainly a duty\",\n                                                         ifelse(V201222 == 3, \"Neither a duty nor a choice\", 0))))\n\nvoting_cd &lt;- voting_cd |&gt; filter(V201225x != -2)\nvoting_cd &lt;- voting_cd |&gt; mutate(V201225x = ifelse(V201225x == 1, \"Very strongly a duty\",\n                                                   ifelse(V201225x == 2, \"Strongly a duty\",\n                                                          ifelse(V201225x == 3, \"Moderately a duty\",\n                                                                 ifelse(V201225x == 4, \"Neutral\",\n                                                                        ifelse(V201225x == 5, \"Moderately a choice\",\n                                                                               ifelse(V201225x == 6, \"Strongly a choice\", \"Very strongly a choice\")))))))\n\nvoting_cd$V201225x &lt;- factor(voting_cd$V201225x, levels = c(\"Very strongly a duty\",\"Strongly a duty\",\n                                                            \"Moderately a duty\",\"Neutral\",\"Moderately a choice\",\n                                                            \"Strongly a choice\", \"Very strongly a choice\"))\n\nvoting_cd &lt;- voting_cd |&gt; mutate(V201075x = ifelse(V201075x == \"Inapplicable\", \"Undeclared\", V201075x))\nvoting_cd$V201075x &lt;- factor(voting_cd$V201075x, levels = c(\"Undeclared\", \"Independent\", \"Republican\",\"Democrat\"))\nvoting_cd$PolSciKnow &lt;- factor(voting_cd$PolSciKnow, levels = c(\"High\", \"Medium\", \"Low\"))\n\nvoting_cd_group1 &lt;- voting_cd |&gt; filter(V201221 != 0)\nvoting_cd_group1 &lt;- voting_cd_group1 |&gt; dplyr::select(V200001, V201075x, PolSciKnow, V201221, V201225x)\n\nvoting_cd_group2 &lt;- voting_cd |&gt; filter(V201222 != 0)\nvoting_cd_group2 &lt;- voting_cd_group2 |&gt; dplyr::select(V200001, V201075x, PolSciKnow, V201222, V201225x)\n\n\n\n\nCode\ndiverging_colors_7 &lt;- brewer.pal(7, \"BrBG\")\n\ndfcs &lt;- ggplot(voting_cd_group1, aes(x = V201075x, fill=V201225x)) +\n  geom_bar(position = \"fill\") +\n  facet_wrap(~PolSciKnow, ncol = 1) +\n  scale_fill_manual(values = diverging_colors_7) +\n  labs(subtitle=\"Duty First, Choice Second\", fill = \"Response\") +\n  theme(plot.subtitle = element_text(hjust = 0.5)) +\n  coord_flip()\n\ncfds &lt;- ggplot(voting_cd_group2, aes(x = V201075x, fill=V201225x)) +\n  geom_bar(position = \"fill\") +\n  facet_wrap(~PolSciKnow, ncol=1) +\n  scale_fill_manual(values = diverging_colors_7) +\n  labs(subtitle=\"Choice First, Duty Second\", fill = \"Response\") +\n  theme(plot.subtitle = element_text(hjust = 0.5)) +\n  coord_flip()\n\n(dfcs + cfds) + plot_layout(guides = \"collect\", axis_title=\"collect\") +\n  plot_annotation(title = \"Is Voting a ... or a ...?\", subtitle = \"The Impact of Leading Questions by Political Science Knowledge\") &\n  labs(x = \"Political Affiliation\", y = \"Proportion\") &\n  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nOn several occasions the questionnaire was equally spliced into two different versions so as to test for the impact of the so-called leading question on the Respondents’ answers. In social sciences, leading questions arise when an inherent ordering associated with a question suggests the answer (usually the one appearing first), which is directly linked to the reading direction in a given language (i.e., as in English one reads left to right, the part of the question to the leftmost of the sentence is hypothesized to be the most suggestive of the answer). Here, consider an example of a leading question on the nature of voting.\nUnsurprisingly, the majority of Americans believe that voting is a duty (hues of brown). Nevertheless, these results are inconsistent when faceted by the ordering (total length of brown bars on the left compared to the right), which suggests that the structure of a question did have an impact on the Respondent’s answer, even when one controls for the level of Political Science Knowledge. For example, Democrats with a High level of Political Science Knowledge were less likely to consider voting a duty if “choice” appeared first in the question. Republicans in the same category were more consistent. At the same time, Independents and Undeclared were more likely to be equally split between the two options, but as there were significantly less individuals falling into either of these groups, the general results are not as robust as they are for the main two parties.\nRespondents with a Medium or Low level of Political Science Knowledge seem to consider voting less of a duty and more of a choice, which is consistent with the assumption that education might make one more politically inclined (which was also illustrated on the Mosaic Plot before). Interestingly, while it still persists, the impact of the leading question was less pronounced in these two groups, which is generally not what one would have expected. A possible explanation is that people falling into the High Political Science Knowledge group might have been overthinking the answer, with a decision being eventually implicitly influenced by the ordering of terms in the question, while those with Medium or Low levels answered whatever they thought of first, with questionnaire’s “persuasive technique” not getting enough time to start working out.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#who-is-more-likely-to-lose-it-to-emotions",
    "href": "results.html#who-is-more-likely-to-lose-it-to-emotions",
    "title": "3  Results",
    "section": "3.4 Who is more likely to lose it to emotions?",
    "text": "3.4 Who is more likely to lose it to emotions?\n\n\nCode\nemotions_1 &lt;- alluvials |&gt; dplyr::select(V200001, V201075x, PolSciKnow)\nemotions_2 &lt;- df |&gt; dplyr::select(V200001, V201115, V201116, V201117, V201117, V201118, V201119, V201120, V201121, V201122, V201123)\nemotions &lt;- full_join(emotions_1, emotions_2, by = \"V200001\")\n\nemotions &lt;- emotions |&gt; filter (V201115 != -9 & V201115 !=-8)\nemotions &lt;- emotions |&gt; filter (V201116 != -9 & V201116 !=-8)\nemotions &lt;- emotions |&gt; filter (V201117 != -9 & V201117 !=-8)\nemotions &lt;- emotions |&gt; filter (V201118 != -9 & V201118 !=-8)\nemotions &lt;- emotions |&gt; filter (V201119 != -9 & V201119 !=-8)\nemotions &lt;- emotions |&gt; filter (V201120 != -9 & V201120 !=-8)\nemotions &lt;- emotions |&gt; filter (V201121 != -9 & V201121 !=-8)\nemotions &lt;- emotions |&gt; filter (V201122 != -9 & V201122 !=-8)\nemotions &lt;- emotions |&gt; filter (V201123 != -9 & V201123 !=-8)\n\nemotions &lt;- emotions |&gt; mutate(V201115 = ifelse(V201115 == 1, \"Not at all\",\n                                                   ifelse(V201115 == 2, \"A little\",\n                                                          ifelse(V201115 == 3, \"Somewhat\",\n                                                                 ifelse(V201115 == 4, \"Very\", \"Extremely\")))))\n\nemotions &lt;- emotions |&gt; mutate(V201116 = ifelse(V201116 == 1, \"Not at all\",\n                                                   ifelse(V201116 == 2, \"A little\",\n                                                          ifelse(V201116 == 3, \"Somewhat\",\n                                                                 ifelse(V201116 == 4, \"Very\", \"Extremely\")))))\n\nemotions &lt;- emotions |&gt; mutate(V201117 = ifelse(V201117 == 1, \"Not at all\",\n                                                   ifelse(V201117 == 2, \"A little\",\n                                                          ifelse(V201117 == 3, \"Somewhat\",\n                                                                 ifelse(V201117 == 4, \"Very\", \"Extremely\")))))\n\nemotions &lt;- emotions |&gt; mutate(V201118 = ifelse(V201118 == 1, \"Not at all\",\n                                                   ifelse(V201118 == 2, \"A little\",\n                                                          ifelse(V201118 == 3, \"Somewhat\",\n                                                                 ifelse(V201118 == 4, \"Very\", \"Extremely\")))))\n\nemotions &lt;- emotions |&gt; mutate(V201119 = ifelse(V201119 == 1, \"Not at all\",\n                                                   ifelse(V201119 == 2, \"A little\",\n                                                          ifelse(V201119 == 3, \"Somewhat\",\n                                                                 ifelse(V201119 == 4, \"Very\", \"Extremely\")))))\n\nemotions &lt;- emotions |&gt; mutate(V201120 = ifelse(V201120 == 1, \"Not at all\",\n                                                   ifelse(V201120 == 2, \"A little\",\n                                                          ifelse(V201120 == 3, \"Somewhat\",\n                                                                 ifelse(V201120 == 4, \"Very\", \"Extremely\")))))\n\nemotions &lt;- emotions |&gt; mutate(V201121 = ifelse(V201121 == 1, \"Not at all\",\n                                                   ifelse(V201121 == 2, \"A little\",\n                                                          ifelse(V201121 == 3, \"Somewhat\",\n                                                                 ifelse(V201121 == 4, \"Very\", \"Extremely\")))))\n\nemotions &lt;- emotions |&gt; mutate(V201122 = ifelse(V201122 == 1, \"Not at all\",\n                                                   ifelse(V201122 == 2, \"A little\",\n                                                          ifelse(V201122 == 3, \"Somewhat\",\n                                                                 ifelse(V201122 == 4, \"Very\", \"Extremely\")))))\n\nemotions &lt;- emotions |&gt; mutate(V201123 = ifelse(V201123 == 1, \"Not at all\",\n                                                   ifelse(V201123 == 2, \"A little\",\n                                                          ifelse(V201123 == 3, \"Somewhat\",\n                                                                 ifelse(V201123 == 4, \"Very\", \"Extremely\")))))\n\nemotions &lt;- emotions|&gt; mutate(V201075x = ifelse(V201075x == \"Inapplicable\", \"Undeclared\", V201075x))\nemotions$V201075x &lt;- factor(emotions$V201075x, levels = c(\"Democrat\", \"Republican\", \"Independent\",\"Undeclared\"))\n\nemotion_scale = as.factor(c(\"Not at all\", \"A little\", \"Somewhat\", \"Very\", \"Extremely\"))\nemotions &lt;- emotions |&gt; mutate(across(c(4:12), ~ factor(.x, levels = emotion_scale)))\ngradual_colors &lt;- brewer.pal(5, \"Greens\")\n\nnames(emotions)[4] &lt;- \"hopeful\"\nnames(emotions)[5] &lt;- \"afraid\"\nnames(emotions)[6] &lt;- \"outraged\"\nnames(emotions)[7] &lt;- \"angry\"\nnames(emotions)[8] &lt;- \"happy\"\nnames(emotions)[9] &lt;- \"proud\"\nnames(emotions)[10] &lt;- \"worried\"\nnames(emotions)[11] &lt;- \"irritated\"\nnames(emotions)[12] &lt;- \"nervous\"\n\nemotions_long &lt;- emotions |&gt; pivot_longer(cols = 4:12, names_to = \"Emotion\", values_to = \"Strength\")\n\n\n\n\nCode\nggplot(emotions_long, aes(x = Emotion, fill = Strength)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(values = gradual_colors) +\n  labs(title=\"How ... do you feel about how things are going in the country?\",\n       fill = \"Response\", y = \"Proportion\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nHeated elections are almost guaranteed to directly translate to emotional choices. Consider first the general distribution of emotions in the population of Respondents who have attempted the questionnaire. The two most striking conclusions are that there are not many people who were either extremely worried or extremely happy about how things were going in the United States before the 2020 elections, which might have reflected the “business as usual” attitude. Almost any other emotion (except for hopeful) represented a roughly equal split among all the possible response categories. Were people falling into any of these groups predicting what was about to happen in November 2020?\n\n\nCode\nemotions_long_high &lt;- emotions_long |&gt; filter(PolSciKnow == \"High\")\n\nhigh &lt;- ggplot(emotions_long_high, aes(x = Emotion, fill = Strength)) +\n  geom_bar(position = \"fill\") +\n  facet_wrap(~V201075x, nrow=1) +\n  scale_fill_manual(values = gradual_colors) +\n  labs(title = \"High Level of Political Science Knowledge\", fill = \"Response\", y = \"Proportion\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  coord_flip()\n\nemotions_long_mid &lt;- emotions_long |&gt; filter(PolSciKnow == \"Medium\")\n\nmid &lt;- ggplot(emotions_long_mid, aes(x = Emotion, fill = Strength)) +\n  geom_bar(position = \"fill\") +\n  facet_wrap(~V201075x, nrow=1) +\n  scale_fill_manual(values = gradual_colors) +\n  labs(title = \"Medium Level of Political Science Knowledge\", fill = \"Response\", y = \"Proportion\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  coord_flip()\n\nemotions_long_low &lt;- emotions_long |&gt; filter(PolSciKnow == \"Low\")\n\nlow &lt;- ggplot(emotions_long_low, aes(x = Emotion, fill = Strength)) +\n  geom_bar(position = \"fill\") +\n  facet_wrap(~V201075x, nrow=1) +\n  scale_fill_manual(values = gradual_colors) +\n  labs(title = \"Low Level of Political Science Knowledge\", fill = \"Response\", y = \"Proportion\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  coord_flip()\n\n(high / mid / low) + plot_layout(guides = \"collect\", axis_title=\"collect\") +\n  plot_annotation(title = \"How ... do you feel about how things are going in the country?\\n Grouped by Political Science Knowledge\") &\n  labs(x = \"Emotion\", y = \"Proportion\") &\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nIt is very much possible! The number of High Political Science Knowledge Democrats being extremely worried was significantly smaller than the number of High Political Science Knowledge Republicans exhibiting extreme worry. Did they know that Joe Biden was to be the called as winner? Unfortunately, this cannot be inferred, but compared to the proportion of Democrats expressing extreme happiness or hopefulness, the proportion of extreme worry does not seem to be awry, especially when compared to the general distributions in the previous graph. Nevertheless, Democrats were much more likely to more strongly express negative emotions (nervous, irritated, outraged) compared to Republicans which seemed to have been more toned in all categories. In other words, while there might have been a light at the end of the tunnel (or the campaign), High Political Science Knowledge Democrats were prevalently dissatisfied with how things were going in the country, compared to the same group of Republicans.\nFor those falling into the Medium or Low Political Science Knowledge category, the trends seem to change only slightly. That is, Democrats are more pronounced about the negative emotions (especially nervousness), while Republicans appear as balanced as they were before. This could hint at the level of worriness being linked to the outcome of the elections, but more targetted analysis would have to be done to confirm such a possibility. For Independents and Undeclared, the trends also appear the same across all different levels of Political Science Knowledge.\nMost importantly, and generally speaking, there seems to be little association between experienced emotions and the level of Political Science Knowledge. In other words, to feel things–and to report it–is to live a life of a human, which was the case for all the Respondents.\n\n\nCode\npositive_emotions &lt;- c(\"hopeful\", \"happy\", \"proud\") \nnegative_emotions &lt;- c(\"afraid\", \"outraged\", \"angry\", \"worried\", \"irritated\", \"nervous\")\n\npositive_emotions_scores &lt;- c(\"Extremely\" = 4, \"Very\" = 3, \"Somewhat\" = 2, \"A little\" = 1, \"Not at all\" = 0)\nnegative_emotions_scores &lt;- c(\"Extremely\" = -4, \"Very\" = -3, \"Somewhat\" = -2, \"A little\" = -1, \"Not at all\" = 0)\n\nemotions_votes &lt;- emotions\n\nemotions_votes &lt;- emotions_votes |&gt;\n  mutate(across(\n    all_of(positive_emotions), \n    ~ positive_emotions_scores[.x]\n  )) |&gt;\n  mutate(across(\n    all_of(negative_emotions), \n    ~ negative_emotions_scores[.x]\n  )) |&gt;\n  rowwise() |&gt;\n  mutate(\n    Positive_Score = sum(c_across(all_of(positive_emotions)), na.rm = TRUE),\n    Negative_Score = sum(c_across(all_of(negative_emotions)), na.rm = TRUE),\n    Total_Score = Positive_Score + Negative_Score\n  ) |&gt;\n  ungroup()\n\nemotions_votes &lt;- emotions_votes |&gt; dplyr::select(V200001, V201075x, PolSciKnow, Total_Score)\nwinners_emotions &lt;- winners |&gt; dplyr::select(V200001, V201217)\nemotions_votes &lt;- full_join(emotions_votes, winners_emotions, by='V200001')\nemotions_votes &lt;- emotions_votes |&gt; drop_na()\n\nemotions_votes &lt;- emotions_votes |&gt; mutate(Match = ifelse(V201075x == \"Republican\" & V201217 == \"Donald Trump\", \"Expectation v. Prediction Convergence\",\n                                           ifelse(V201075x == \"Republican\" & V201217 == \"Joe Biden\", \"Expectation v. Prediction Divergence\",\n                                                 ifelse(V201075x == \"Democrat\" & V201217 == \"Donald Trump\", \"Expectation v. Prediction Divergence\", \"Expectation v. Prediction Convergence\"))))\n\nemotions_democrats &lt;- emotions_votes |&gt; filter(V201075x == \"Democrat\")\nemotions_republicans &lt;- emotions_votes |&gt; filter(V201075x == \"Republican\")\n\n\n\n\nCode\ne_d &lt;- ggplot(emotions_democrats, aes(x=Total_Score)) +\n  geom_histogram(bins = 24, binwidth = 1, fill = \"blue\", color = \"black\", alpha = 0.33) +\n  facet_wrap(~Match, ncol=1, scales = \"free_y\") +\n  labs(title = \"Democrats\")\n\ne_r &lt;- ggplot(emotions_republicans, aes(x=Total_Score)) +\n  geom_histogram(bins = 24, binwidth = 1, fill = \"red\", color = \"black\", alpha = 0.33) +\n  geom_density() +\n  facet_wrap(~Match, ncol=1, scales = \"free_y\") +\n  labs(title = \"Republicans\")\n\n(e_d + e_r) + plot_layout(guides = \"collect\", axis_title=\"collect\") +\n  plot_annotation(title = \"Position on the Emotional Spectrum by Party Affiliation\\nand Relation between Respondent's Expectations and Predictions\") &\n  labs(x = \"Emotional Spectrum Score\", y = \"Count\") &\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nAs there seems to be a difference between the emotions reported by Democrat-voting and Republican-voting Respondents, a natural next step is to test for any association in the balance between positive and negative emotions and the “gut” feelings about the winner. In short, is there a difference in distribution of emotional spectrum scores when faceted by the mismatch between the expectation and prediction on the 2020 Presidential election winner.\nFirst, a note on methodology. The emotional spectrum score was calculated by contrasting the scope of a positive push of positive emotions (happy, hopeful, proud) and a negative push of the remaining negative emotions (worried, outraged, nervous, irritated, angry, afraid). Expectation references a winner which is compliant with one’s party affiliation, while prediction references a winner which is called based on what one thinks on who the winner will be at the national level. Thus, convergence means that there is a match between the two, and divergence means that there is a mismatch (e.g., a Republican thinks Joe Biden will win). Do these groups behave differently? YES!\nBeyond the typical Democrat-Republican divide, which is additionally strengthened by a different distribution of emotional spectrum scores, the Kolmogorov-Smirnov tests point out a significant difference between the in-party groups.\n\n\nCode\nd_convergence &lt;- emotions_democrats |&gt; filter(Match == \"Expectation v. Prediction Convergence\") |&gt; dplyr::select(Total_Score)\nd_convergence &lt;- d_convergence[['Total_Score']]\nd_divergence &lt;- emotions_democrats |&gt; filter(Match == \"Expectation v. Prediction Divergence\") |&gt; dplyr::select(Total_Score)\nd_divergence &lt;- d_divergence[['Total_Score']]\n\nr_convergence &lt;- emotions_republicans |&gt; filter(Match == \"Expectation v. Prediction Convergence\") |&gt; dplyr::select(Total_Score)\nr_convergence &lt;- r_convergence[['Total_Score']]\nr_divergence &lt;- emotions_republicans |&gt; filter(Match == \"Expectation v. Prediction Divergence\") |&gt; dplyr::select(Total_Score)\nr_divergence &lt;- r_divergence[['Total_Score']]\n\nks.test(d_convergence,d_divergence)\n\n\nWarning in ks.test.default(d_convergence, d_divergence): p-value will be\napproximate in the presence of ties\n\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  d_convergence and d_divergence\nD = 0.1495, p-value = 6.303e-08\nalternative hypothesis: two-sided\n\n\nCode\nks.test(r_convergence,r_divergence)\n\n\nWarning in ks.test.default(r_convergence, r_divergence): p-value will be\napproximate in the presence of ties\n\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  r_convergence and r_divergence\nD = 0.058554, p-value = 0.66\nalternative hypothesis: two-sided\n\n\nIn plain English, the results say that Democrats falling into the Divergence group are significantly different from Democrats falling into the Convergence group, while for Republicans there is not enough evidence to conclude a difference between the Divergence and Convergence group. Working out these results backwards, confirms the findings from the previous plot, where Republicans were reported to be quite balanced emotionally. On the other hand, Democrats who fear that Joe Biden might lose tend to report more negative emotions than Democrats who think that Joe Biden will win (it is reflected by the peak around the score of -8). The violation of normality in the case of Democrats comes from the smaller number of Respondents affiliated with this party reporting positive emotions in general, while the maintenance of normality for Republicans also testifies to their perceived emotional balance.\nTwo more takeaways worth further consideration: 1) the mean of any distribution above is negative, which means that Respondents are generally more likely to experience negative emotions more frequently than positive ones; 2) thinking about the events of January 2021 that happened on the Capitol Hill, it is a bit surprising to find exactly the opposite trend in the data, i.e., that Democrats facing unmet expectations are predicted to behave differently. Is it why nobody could predict the invasion of the Capitol building?\nOr, perhaps, different events speak to different emotions of Democrats and Republicans, triggering a different scope of reactions (say, experiencing a rise in negative emotions vs. starting an insurgency). However, the former might be a bit more easily influenceable if a vision of a losing Democrat candidate is successfully implanted in their minds.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#what-do-the-biases-of-the-political-compass-say-about-the-american-voters",
    "href": "results.html#what-do-the-biases-of-the-political-compass-say-about-the-american-voters",
    "title": "3  Results",
    "section": "3.5 What do the biases of the Political Compass say about the American Voters?",
    "text": "3.5 What do the biases of the Political Compass say about the American Voters?\n\n\nCode\npolitical_spectrum_1 &lt;- alluvials |&gt; dplyr::select(V200001, V201075x, PolSciKnow)\npolitical_spectrum_2 &lt;- df |&gt; dplyr::select(V200001, V201200, V201201, V201202, V201203)\n\npolitical_spectrum &lt;- full_join(political_spectrum_1, political_spectrum_2, by='V200001')\n\npolitical_spectrum &lt;- political_spectrum |&gt; filter(V201200 != -9 & V201200 != -8)\npolitical_spectrum &lt;- political_spectrum |&gt; mutate(V201200 = ifelse(V201200 == 99 & V201201 == 1, 2,\n                                                                    ifelse(V201200 == 99 & V201201 == 2, 6,\n                                                                           ifelse(V201200 == 99 & V201201 == 3, 4, V201200))))\npolitical_spectrum &lt;- political_spectrum |&gt; mutate(V201200 = ifelse(V201200 == 99, -1, V201200))\n\npolitical_spectrum &lt;- political_spectrum |&gt; mutate(V201202 = ifelse(V201202 == -9 | V201202 == -8, -1, V201202))\npolitical_spectrum &lt;- political_spectrum |&gt; mutate(V201203 = ifelse(V201203 == -9 | V201203 == -8 | V201203 == -4, -1, V201203))\n\npolitical_spectrum &lt;- political_spectrum |&gt; mutate(V201075x = ifelse(V201075x == \"Inapplicable\", \"Independent\", V201075x))\npolitical_spectrum$V201075x &lt;- factor(political_spectrum$V201075x, levels = c(\"Independent\", \"Republican\",\"Democrat\"))\npolitical_spectrum$PolSciKnow &lt;- factor(political_spectrum$PolSciKnow, levels = c(\"High\", \"Medium\", \"Low\"))\n\npolitical_spectrum_biden &lt;- political_spectrum |&gt;\n  group_by(V201075x, PolSciKnow, V201200, V201202) |&gt;\n  summarize(Count = n(), .groups = \"keep\")\npolitical_spectrum_biden &lt;- political_spectrum_biden |&gt;\n  group_by(PolSciKnow, V201200, V201202) |&gt;\n  slice_max(Count, n = 1, with_ties = FALSE) |&gt; ungroup()\npolitical_spectrum_biden &lt;- political_spectrum_biden |&gt;\n  mutate(Count = ifelse(V201075x == \"Republican\", -Count, \n                        ifelse(V201075x == \"Independent\", 0, Count)))\n\npolitical_spectrum_trump &lt;- political_spectrum |&gt;\n  group_by(V201075x, PolSciKnow, V201200, V201203) |&gt;\n  summarize(Count = n(), .groups = \"keep\")\npolitical_spectrum_trump &lt;- political_spectrum_trump |&gt;\n  group_by(PolSciKnow, V201200, V201203) |&gt;\n  slice_max(Count, n = 1, with_ties = FALSE) |&gt; ungroup()\npolitical_spectrum_trump &lt;- political_spectrum_trump |&gt;\n  mutate(Count = ifelse(V201075x == \"Republican\", -Count, \n                        ifelse(V201075x == \"Independent\", 0, Count)))\n\n\n\n\nCode\nx_labels = factor(c(\"No Answer\", \"Extremely\\nLiberal\", \"Liberal\", \"Slightly\\nLiberal\", \"Moderate\", \"Slightly\\nConservative\", \"Conservative\", \"Extremely\\nConservative\"))\ny_labels = factor(c(\"No Answer\", \"Extremely\\nLiberal\", \"Liberal\", \"Slightly\\nLiberal\", \"Moderate\", \"Slightly\\nConservative\", \"Conservative\", \"Extremely\\nConservative\"))\n\nbiden &lt;- ggplot(political_spectrum_biden, aes(x=V201200, y=V201202, fill = Count)) +\n  geom_tile(color = \"white\") +\n  facet_wrap(~PolSciKnow, ncol=1) +\n  scale_fill_gradient2(low = \"red\",mid = \"#f9f9f9\",high = \"blue\",midpoint = 0, name = \"Party Prevalence\\n(Counts)\") +\n  scale_x_continuous(breaks = c(-1, 1, 2, 3, 4, 5, 6, 7), labels = x_labels) + \n  scale_y_continuous(breaks = c(-1, 1, 2, 3, 4, 5, 6, 7), labels = y_labels) +\n  labs(y = \"Joe Biden's Position\", title = \"Respondent v. Joe Biden\") +\n  theme(plot.title = element_text(hjust = 0.5), legend.position=\"none\")\n\ntrump &lt;- ggplot(political_spectrum_trump, aes(x=V201200, y=V201203, fill = Count)) +\n  geom_tile(color = \"white\") +\n  facet_wrap(~PolSciKnow, ncol=1) +\n  scale_fill_gradient2(low = \"red\",mid = \"#f9f9f9\",high = \"blue\",midpoint = 0, name = \"Party Prevalence\\n(Counts)\") +\n  scale_x_continuous(breaks = c(-1, 1, 2, 3, 4, 5, 6, 7), labels = x_labels) + \n  scale_y_continuous(breaks = c(-1, 1, 2, 3, 4, 5, 6, 7), labels = y_labels) +\n  labs(y = \"Donald Trump's Position\", title = \"Respondent v. Donald Trump\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n(biden + trump) + plot_layout(guides = \"collect\", axis_title=\"collect\") +\n  plot_annotation(title = \"Political Compass by Political Science Knowledge\") &\n  labs(x = \"Respondent's Position\") &\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nA political spectrum scale (a single-axis case of the Political Compass) is arguably the most common metric used in the classification of individuals on the left-right liberal-conservative spectrum. In the case of a two-party system (like the one in the United States), it usually links these two parties to the opposite ends in a relatively straightforward and direct manner. Nevertheless, there are certain known biases associated with this type of mapping, one of which is the lack of independence in the sense that the reported results are highly sensitive to the (political) partiality of inputs. How is this reflected in the population of American Voters?\nConsider these six heatmaps which on the x-axes position the Respondent on the said scale (self-reported) and on the y-axes position Joe Biden and Donald Trump thereupon, respectively. If a given combination was more likely to be reported by Democrat-voting Respondents, the tile has been colored blue, and it was colored red if the combination was more likely reported by Republican-voting Respondents. Light gray tiles represent combinations which were either not frequently reported by either party or were associated with Independents and Undeclared voters. Everything has been also faceted by the level of Political Science Knowledge.\nFirst, the vertical spread of values suggests that voters are divided in relation to positioning the presidential candidate on the political spectrum scale. The horizontal spread of values reflects the distribution of the electorate, split by party affiliation. It is highly evident that at all levels of Political Science Knowledge, there is never a match between the darkest blue and darkest red tile on the same horizontal level per heatmap. In other words, Democrat-voting and Republican-voting Respondents never agree on the position of the opposite party candidate. Both tend to exaggerate the opponent’s position, i.e., Republicans would say that Joe Biden is extremely liberal, while Democrats would just call Joe Biden liberal, and Democrats would say that Donald Trump is extremely conservative, while Democrats would just call Donald Trump conservative. In other words, the direction seems to be same, but the level of extremeness is not so anymore. This effect does not seem to diminish when one control for the level of Political Science Knowledge.\nThen, while the Democratic-voters seem to be a bit more spread out in both directions (illustrated with the 9-tile blue squares that can be seen in almost any map), the Republican-voters are more homogeneous and clearly centered around the darkest tile, creating more of a cross formation. Any further deviations are likely suggestive of uninformation, noise, or unseriousness in filling the questionnaire. For example, the prevalence of blue in the bottom-left heatmap around the self:conservative, Donald Trump:extremely liberal combination is highly unjustified and incomprehensible in relation to reality as it means that one might not have understood the functioning of the political spectrum scale. The same holds true for the bottom-right heatmap exhibiting traces of red for self:liberal, Joe Biden:extremely conservative combination. In other words, there are numbers of people who do not differentiate between Liberalism and Conservatism, which, fortunately, seems to be filtered quite well when one applies the Political Science Knowledge thresholds.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#are-voters-aware-of-the-economic-performance-indicators-at-the-national-level",
    "href": "results.html#are-voters-aware-of-the-economic-performance-indicators-at-the-national-level",
    "title": "3  Results",
    "section": "3.6 Are voters aware of the economic performance indicators at the national level?",
    "text": "3.6 Are voters aware of the economic performance indicators at the national level?\n\n\nCode\ncleveland_1 &lt;- alluvials |&gt; dplyr::select(V200001, V201075x, PolSciKnow)\ncleveland_2 &lt;- winners |&gt; dplyr::select(V200001, V201014b)\ncleveland_3 &lt;- df |&gt; dplyr::select(V200001, V201327x, V201333x)\n\ncleveland &lt;- full_join(cleveland_1, cleveland_2, by='V200001')\ncleveland &lt;- full_join(cleveland, cleveland_3, by='V200001')\n\ncleveland &lt;- cleveland |&gt; drop_na()\n\ncleveland$V201075x &lt;- factor(cleveland$V201075x, levels = c(\"Democrat\", \"Republican\"))\n\ncleveland$PolSciKnow &lt;- factor(cleveland$PolSciKnow, levels = c(\"High\", \"Medium\", \"Low\"))\n\ncleveland &lt;- cleveland |&gt; filter(V201327x != -2)\ncleveland &lt;- cleveland |&gt; mutate(V201327x = ifelse(V201327x == 1 | V201327x == 2, \"Better\",\n                                                   ifelse(V201327x == 3, \"Same\", \"Worse\")))\n\ncleveland &lt;- cleveland |&gt; filter(V201333x != -2)\ncleveland &lt;- cleveland |&gt; mutate(V201333x = ifelse(V201333x == 1 | V201333x == 2, \"Better\",\n                                                   ifelse(V201333x == 3, \"Same\", \"Worse\")))\n\ncleveland$V201014b &lt;- factor(cleveland$V201014b, levels = c(\"Alabama\",\"Alaska\",\"Arizona\",\"Arkansas\",\n                                                            \"California\",\"Colorado\",\"Connecticut\",\"Delaware\",\n                                                            \"DC\",\"Florida\",\"Georgia\",\"Hawaii\",\"Idaho\",\"Illinois\",\n                                                            \"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\n                                                            \"Maine\",\"Maryland\",\"Massachusetts\",\"Michigan\",\n                                                            \"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\n                                                            \"Nebraska\",\"Nevada\",\"New Hampshire\",\"New Jersey\",\n                                                            \"New Mexico\",\"New York\",\"North Carolina\",\"North Dakota\",\n                                                            \"Ohio\",\"Oklahoma\",\"Oregon\",\"Pennsylvania\",\"Rhode Island\",\n                                                            \"South Carolina\",\"South Dakota\",\"Tennessee\",\"Texas\",\"Utah\",\n                                                            \"Vermont\",\"Virginia\",\"Washington\",\"West Virginia\",\n                                                            \"Wisconsin\",\"Wyoming\"))\n\n\n\n\nCode\ncleveland_economy_know &lt;- cleveland |&gt;\n  group_by(PolSciKnow, V201075x, V201014b, V201327x) |&gt;\n  summarize(Proportion = n(), .groups = \"keep\")\n\ncleveland_economy_know &lt;- cleveland_economy_know |&gt;\n  group_by(V201014b, V201075x, PolSciKnow, V201327x) |&gt;\n  summarize(Total = sum(Proportion), .groups = \"drop\") |&gt;\n  group_by(V201014b, V201075x, PolSciKnow) |&gt;\n  mutate(Prop = Total / sum(Total)) |&gt;\n  ungroup()\n\naverages &lt;- cleveland_economy_know |&gt;\n  group_by(V201075x, PolSciKnow, V201327x) |&gt;\n  summarize(Average = mean(Prop), .groups = \"drop\")\ncleveland_economy_know &lt;- cleveland_economy_know |&gt;\n  left_join(averages, by = c(\"V201075x\", \"PolSciKnow\", \"V201327x\"))\n\nhigh_know &lt;- cleveland_economy_know |&gt; filter(PolSciKnow == \"High\")\nmid_know &lt;- cleveland_economy_know |&gt; filter(PolSciKnow == \"Medium\")\nlow_know &lt;- cleveland_economy_know |&gt; filter(PolSciKnow == \"Low\")\n\nhigh_graph &lt;- ggplot(high_know, aes(x=Prop, y=fct_rev(V201014b), color=V201075x)) +\n  geom_point() +\n  geom_vline(aes(xintercept = Average, color = V201075x)) +\n  facet_wrap(~V201327x) +\n  scale_color_manual(values = c(\"Democrat\" = \"blue\", \"Republican\" = \"red\")) +\n  labs(title = \"High Level of Political Science Knowledge\")\n\nmid_graph &lt;- ggplot(mid_know, aes(x=Prop, y=fct_rev(V201014b), color=V201075x)) +\n  geom_point() +\n  facet_wrap(~V201327x) +\n  geom_vline(aes(xintercept = Average, color = V201075x)) +\n  scale_color_manual(values = c(\"Democrat\" = \"blue\", \"Republican\" = \"red\")) +\n  labs(title = \"Medium Level of Political Science Knowledge\")\n\nlow_graph &lt;- ggplot(low_know, aes(x=Prop, y=fct_rev(V201014b), color=V201075x)) +\n  geom_point() +\n  facet_wrap(~V201327x) +\n  geom_vline(aes(xintercept = Average, color = V201075x)) +\n  scale_color_manual(values = c(\"Democrat\" = \"blue\", \"Republican\" = \"red\")) +\n  labs(title = \"Low Level of Political Science Knowledge\")\n\n(high_graph / mid_graph / low_graph) + plot_layout(guides = \"collect\", axis_title=\"collect\") +\n  plot_annotation(title = \"In the past year, has the Nation’s Economy Gotten Better, Stayed About the Same, or Gotten Worse?\") &\n  labs(x = \"Proportion\", y=\"\", color = \"Party Affiliation\") &\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nA number of the most important economic indicators, such as the rate of economy’s growth or the rate of unemployment, are said to be extremely influential on the outcome of any elections. To begin with, consider these Cleveland Dot Plots which report the proportion of Respondents, split by their preferred party and current state of residence, answering better, same, or worse to the question on the nation’s economy performance in the past 12 months, i.e., the period between November 2019 and November 2020. The correct answer can be either “worse” or “same” as the American GDP has fallen by a very small fraction as a result of the COVID-19 pandemic.\nOn a technical note, the reported results are linearly dependent, which means that the proportions for each horizontal line shall add exactly to 1. Thus, any two facets directly inform on the third one. Moreover, for the simplicity of analyses, the vertical lines represent the party averages across all states.\nDemocrats are, on average, more likely than Republicans to say that the economy has gotten worse. At the same time, Republicans are, on average, more likely than Democrats to say that the economy has stayed the same. While for the “worse” case, the difference between the average proportion stays more less the same across all possible levels of Political Science Knowledge, it is no longer the case for the “same” case, also considered vertically across plots. There, the higher the level of Political Science Knowledge of Republican-voters, the further away they stray from the average for the Democrats claiming similarly at the same level of Political Science Knowledge cognizance. This is as expected with the hypothesis that the higher one’s level of Political Science Knowledge is (which is likely related to the level of Economic Knowledge), the better knowledge one would have on the economic indicators in the country.\nAt the same time, one always seems to exercise caution in praising or criticizing the incumbent president, who might be directly responsible for the nation’s economic performance, which is true especially if they belong to the same party. Thus, the Republican-Democrat difference for the “worse” case is not as striking as the growing inter-party divergence in the “same” case. In other words, Republicans with a higher level of Political Science Knowledge are slightly more likely to more overtly give an impartial opinion on the economy compared to Republicans with a lower level of said knowledge, while still staying on the more cautious side when it comes to criticizing the contemporaneous president (Donald Trump).\n\n\nCode\ncleveland_unemployment_know &lt;- cleveland |&gt;\n  group_by(PolSciKnow, V201075x, V201014b, V201333x) |&gt;\n  summarize(Proportion = n(), .groups = \"keep\")\n\ncleveland_unemployment_know &lt;- cleveland_unemployment_know |&gt;\n  group_by(V201014b, V201075x, PolSciKnow, V201333x) |&gt;\n  summarise(Total = sum(Proportion), .groups = \"drop\") |&gt;\n  group_by(V201014b, V201075x, PolSciKnow) |&gt;\n  mutate(Prop = Total / sum(Total)) |&gt;\n  ungroup()\n\naverages_u &lt;- cleveland_unemployment_know |&gt;\n  group_by(V201075x, PolSciKnow, V201333x) |&gt;\n  summarize(Average = mean(Prop), .groups = \"drop\")\ncleveland_unemployment_know &lt;- cleveland_unemployment_know |&gt;\n  left_join(averages_u, by = c(\"V201075x\", \"PolSciKnow\", \"V201333x\"))\n\nhigh_know_u &lt;- cleveland_unemployment_know |&gt; filter(PolSciKnow == \"High\")\nmid_know_u &lt;- cleveland_unemployment_know |&gt; filter(PolSciKnow == \"Medium\")\nlow_know_u &lt;- cleveland_unemployment_know |&gt; filter(PolSciKnow == \"Low\")\n\nhigh_graph_u &lt;- ggplot(high_know_u, aes(x=Prop, y=fct_rev(V201014b), color=V201075x)) +\n  geom_point() +\n  facet_wrap(~V201333x) +\n  geom_vline(aes(xintercept = Average, color = V201075x)) +\n  scale_color_manual(values = c(\"Democrat\" = \"blue\", \"Republican\" = \"red\")) +\n  labs(title = \"High Level of Political Science Knowledge\")\n\nmid_graph_u &lt;- ggplot(mid_know_u, aes(x=Prop, y=fct_rev(V201014b), color=V201075x)) +\n  geom_point() +\n  facet_wrap(~V201333x) +\n  geom_vline(aes(xintercept = Average, color = V201075x)) +\n  scale_color_manual(values = c(\"Democrat\" = \"blue\", \"Republican\" = \"red\")) +\n  labs(title = \"Medium Level of Political Science Knowledge\")\n\nlow_graph_u &lt;- ggplot(low_know_u, aes(x=Prop, y=fct_rev(V201014b), color=V201075x)) +\n  geom_point() +\n  facet_wrap(~V201333x) +\n  geom_vline(aes(xintercept = Average, color = V201075x)) +\n  scale_color_manual(values = c(\"Democrat\" = \"blue\", \"Republican\" = \"red\")) +\n  labs(title = \"Low Level of Political Science Knowledge\")\n\n(high_graph / mid_graph / low_graph) + plot_layout(guides = \"collect\", axis_title=\"collect\") +\n  plot_annotation(title = \"In the past year, has the Nation's Unemployment Gotten Better, Stayed About the Same, or Gotten Worse?\") &\n  labs(x = \"Proportion\", y=\"\", color = \"Party Affiliation\") &\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nHow about the second important indicator, i.e., unemployment? Here the answer is undeniably “worse” as the COVID-19 pandemic has spiked the unemployment rate from about 3% to almost 15% in the considered period (again, November 2019 to November 2020). Yet, there does not seem to be any different trend compared to the plot reporting the same metrics but on the Nation’s economy. Once more, Democrats are more likely to report “worse” at all levels of Political Science Knowledge, and Republicans diverge from Democrats when it comes to reporting “same” and controlling for the level of Political Science Knowledge.\nSetting aside the discussion on the difference between the “true” and reported answers on these economic indicators, the two Cleveland Dot Plots appear to quite succinctly summarize a number of findings as well as hindrances characterizing this project. For example, while the “same” instance Republican divergence case might hint at a higher level of information among the voters with a higher level of Political Science Knowledge, it is impossible to say just from these plots whether the reported differences in averages are a result of emotionally galvanized electorates, or the lack of actual knowledge. Similarly, one can only hypothesize whether Democrats were truly aware of the worse-performing economic indicators or if they more prevalently chose “worse” in a way to signal their dissatisfaction with the then-currently incumbent president belonging to the opposite party.\nOn a final note, data granularity here has been stretched to its maximum, in the sense that 312 groups were created to split the over 8,000 subjects into their respective Political Science Knowledge - Party Affiliation - State of Residence bin. Thus, a careful reader might notice that in some states there are no Respondents falling into one of the possible groups; for example, all Democrat-voting individuals in Alabama with a High Political Science Knowledge claim that the Nation’s unemployment has gotten worse, resulting in a perfect prediction. Finding such trends, however, requires a line-by-line analysis that might be eclipsed by the inclusion of vertical lines reporting the averages for a simplified subset of 18 groups. Moreover, the robustness of such an approach might not be entirely accurate as it would raise questions on the considered sample size.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "d3graph.html",
    "href": "d3graph.html",
    "title": "4  Interactive graph",
    "section": "",
    "text": "US Government Quiz\n\n\n  &lt;div class=\"question\"&gt;\n      &lt;label for=\"party-senate\"&gt;1. Which party has the most members in Senate right now?&lt;/label&gt;\n      &lt;select id=\"party-senate\"&gt;&lt;/select&gt;\n  &lt;/div&gt;\n  &lt;div class=\"question\"&gt;\n      &lt;label for=\"party-congress\"&gt;2. Which party has the most members in Congress right now?&lt;/label&gt;\n      &lt;select id=\"party-congress\"&gt;&lt;/select&gt;\n  &lt;/div&gt;\n  &lt;div class=\"question\"&gt;\n      &lt;label for=\"senator-term\"&gt;3. How many years are there in one full term of a US Senator?&lt;/label&gt;\n      &lt;input type=\"number\" id=\"senator-term\" placeholder=\"Answer in number\"&gt;\n  &lt;/div&gt;\n  &lt;div class=\"question\"&gt;\n      &lt;label for=\"least-spent\"&gt;4. On which program does the Federal Government spend the least?&lt;/label&gt;\n      &lt;select id=\"least-spent\"&gt;&lt;/select&gt;\n  &lt;/div&gt;\n  &lt;div class=\"question\"&gt;\n      &lt;label for=\"geer-case\"&gt;5. When did the Supreme Court decide on Geer v. Connecticut?&lt;/label&gt;\n      &lt;input type=\"number\" id=\"geer-case\" placeholder=\"Answer in number\"&gt;\n  &lt;/div&gt;\n  &lt;div class=\"question\"&gt;\n      &lt;label for=\"party-select\"&gt;What is your party affiliation? (Republicans or Democrats)&lt;/label&gt;\n      &lt;select id=\"party-select\"&gt;\n          &lt;option value=\"\" disabled selected&gt;Select your party&lt;/option&gt;\n          &lt;option value=\"republicans\"&gt;Republicans&lt;/option&gt;\n          &lt;option value=\"democrats\"&gt;Democrats&lt;/option&gt;\n      &lt;/select&gt;\n  &lt;/div&gt;\n  &lt;button id=\"submit-btn\"&gt;Submit Answers&lt;/button&gt;\n  &lt;div class=\"result\" id=\"result\"&gt;&lt;/div&gt;",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interactive graph</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "5  Conclusion",
    "section": "",
    "text": "On the highest level, this project attempted to provide an answer confirming or denying the existence of uninformation in the U.S. 2020 pre-election data. For this sake, uninformation has been defined as any deviation from the rational choice theory, i.e., a set of choices motivated by biases, lack of access to information, or emotions—in short, any case where a decision would have been different if one was better informed on the possible scope of actions. At each stage, an attempt was made to differentiate Respondents based on their declared party affiliation, eventually connecting the obtained results to the outcome of the 2020 Presidential Race.\nTo test for the robustness of the performed analysis, a specific approach had to be drafted to transcend data obtained from the rather typical question on the Respondent’s highest level of attained education. This was the rationale for introducing the Political Science Knowledge metric obtained from the five questions testing one’s cognizance of politics. The move was to ensure that uninformation does not invade the analysis from the beginning onward, as one might lie (presumably introducing a positive shock) when self-reporting the highest level of attained education. Interestingly, the alluvial diagram of respondents’ performance on the Political Knowledge Catch questions hinted at various individuals likely attempting to cheat by correctly answering a tricky question but missing simpler ones.\nA few conclusions appeared to be consistent with the generally recognized theory in political science. For example, Respondents with university degrees were more likely to exhibit a higher level of political awareness. Self-reporting turned out to be less reliable than reporting on others, which was the best predictor of calling a presidential race at the country’s level. Similarly, leading questions were shown to have impacted the collected answers; polarization was found to have permeated the reported emotions, and several well-established and commonly used metrics were confirmed to be purely relational.\nNevertheless, while some of these irrational trends could be easily illustrated with the available data, establishing causal relations was rarely possible due to the number of potential explanations driving each result. Moreover, clearly defining the nature of a given instance of uninformation was often impossible, especially if a survey question was not drafted initially to test for the hypothesized difference (e.g., questionnaire splices challenging the impact of leading questions). For this reason, one can only attempt to show a specific phenomenon’s existence without explaining the mechanisms operating behind the scenes. This problem was most evident in the polarization heatmaps and economic indicator Cleveland dot plots, where certain conclusions were relatively freely extrapolated from other findings reported in this project or from common sense.\nThe project could certainly be taken to a higher level if one particular instance of uninformation—say, emotions, and not rationality, driving choices—was to be investigated in depth from all the possible perspectives. A somewhat simpler direction for future research would be to consider the U.S. 2020 post-election data to see if the described trends also hold there and available data for any previous presidential election year (for example, 2016 or 2012). Such a panel approach could track the evolution of trends and possibly rule out the cases of using the surveys to express dissatisfaction fueled by a mismatch between one’s political affiliation and the political affiliation of the incumbent president—where these possible combinations are guaranteed to exist in a comparison between 2016 and 2020 data. A valuable future input could also be achieved by designing methods, tools, or algorithms that could impartially tackle and tame relativity in data on uninformation, i.e., a difference in interpretation based on the accepted stand.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  }
]