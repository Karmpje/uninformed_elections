[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Biases in U.S. Elections Data",
    "section": "",
    "text": "1 Introduction\nThe concept of polarization is a long-standing and well-known phenomenon in political sciences, especially relevant to democratic government systems. In plain English, polarization refers to the division of society into distinct groups based on differing agendas, beliefs, opinions, goals, needs, and priorities, which are expressed and debated as part of the democratic process. As a result, these divisions often shape the direction of legislation and policy drafting as opposing factions attempt to advance their values, ethics, and moral frameworks through executive governance.\nThe most common examples of polarization occur along ideological divides, such as the left and right political spectrum (e.g., liberalism and conservatism), religion (secularism and theocracy), economics (communism and capitalism), and social policy (libertarianism and authoritarianism). In countries with a two-party system (an example of which is the United States), the polarizing divide predominantly matches the tensions of partisan identities.\nWhile polarization occurs in any democracy, an interesting detail unique to the United States is the persistence and stability of these societal and political divides. In other words, voters, states, and politicians often adhere strictly to the party lines at any given decision time. Deviations from the stable voting trajectory patterns are rare but highly consequential, usually reflecting major societal dissatisfaction. Moreover, these abnormalities are immensely powerful in turning the scales, as it is safe to assume that each party has a relatively equal electorate population. Thus, while statistically almost improbable, U.S. history is abundant in examples of a single vote tipping the election balance (see, for example, https://middletonma.gov/303/The-Power-of-One-Vote).\nThis project seeks to examine cases where electoral outcomes diverge from established partisan norms or reported public opinion. Identifying these nonconformities could shed light on the presence and impact of structural misinformation or disinformation within American society. Per rational choice theory (i.e., justified decision-making), any such divergence shall be deemed a priori irrational, for it does not reflect true and honest communication of voter preferences. Thus, understanding these gaps could provide insights into the mechanisms driving such irregularities and offer a deeper understanding of voter priorities extending beyond the traditional party lines.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data",
    "section": "",
    "text": "2.1 Description\nEvery four years, matching the pattern of presidential elections in the United States, two studies are conducted in the pre-election and post-election periods, with the November Election Day holiday marking the turning point between the two. Each study interviews respondents online, by phone, and through live video software before the voting day. Then, it attempts to re-interview as many of the same respondents as possible after the voting day. Phone and video interviews are facilitated by trained interviewers who belong to ANES.\nRaw datasets (with the exclusion of sensitive variables that could identify particular respondents) are available for free in multiple formats, including the .csv one, on the ANES website. These are files with data frames characterized by huge dimensionality (but relatively small file size), where the number of rows (representing respondents) oscillates between ~6-8k, and the number of variables is around ~1.8k. Thus, importing the data is relatively simple as it relies on downloading and reading the respective .csv file. For the 2020 Time Series Study election data (analyzed in the missing values section), there are 8280 respondents and 1771 variables. Some variables are categorical (e.g., yes/no questions), and some are numerical (e.g., age). Sampling weights allowing for correct scalability of results to the entire U.S. voting population are also given.\nVariables reflect the two study periods; pre-election variables start with the code “V201,” and post-election variables start with the code “V202.” The exact code-to-question mapping, as well as a thorough description of the methodology covering possible values per question, is accessible in the accompanying “User Guide and Codebook” published alongside each dataset on the ANES website. It is said to be one of the largest datasets on voting, public opinions, and voter tendencies in the United States.\nIn these datasets, missing values are denoted with “-9” or “-8” codes. The former captures skipped questions and answers that the respondent declined to give; it is also the most prevalent. The latter is less frequent as it encodes the explicit answer “I don’t know,” which might have been given after the respondent was prompted with a specific question. Nevertheless, while missing values conceal some data trends, the sheer number of available variables proves the most challenging to deal with, as any analysis requires a careful variable selection or dimensionality reduction to be performed first. Otherwise, irrelevant factors (noise) might be erroneously considered.\nFull Citation (again, specifically for the 2020 data): American National Election Studies. 2021. ANES 2020 Time Series Study Full Release [dataset and documentation]. February 10, 2022 version. www.electionstudies.org",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2  Data",
    "section": "2.2 Missing value analysis",
    "text": "2.2 Missing value analysis\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(forcats)\n\n\n\n\nCode\n# loading the dataset\nanes_data &lt;- read_csv(\"data/raw/anes_timeseries_2020_csv_20220210.csv\", show_col_types = FALSE)\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nCode\n# selecting variables of interest\npre_columns &lt;- grep(\"^V201(00[1-9]|0[1-9][0-9]|[1-5][0-9][0-9]|6[0-5][0-8])$\", colnames(anes_data), value = TRUE)\npre_election_data &lt;- anes_data[, pre_columns]\npost_columns &lt;- grep(\"^V202(00[1-9]|0[1-9][0-9]|[1-6][0-4][0-5])$\", colnames(anes_data), value = TRUE)\npost_election_data &lt;- anes_data[, post_columns]\n\n# filtering for NAs encoded as -9\npre_columns_with_negative_nine &lt;- sapply(pre_election_data, function(x) any(x == -9, na.rm = TRUE))\npre_missing_data &lt;- pre_election_data[, pre_columns_with_negative_nine]\npost_columns_with_negative_nine &lt;- sapply(post_election_data, function(x) any(x == -9, na.rm = TRUE))\npost_missing_data &lt;- post_election_data[, post_columns_with_negative_nine]\n\n# calculating the proportions of missing data and setting the threshold\nthreshold &lt;- 0.02\nproportion_negative_nine_pre &lt;- sapply(pre_missing_data, function(x) mean(x == -9))\npre_sorted_proportions &lt;- sort(proportion_negative_nine_pre, decreasing = TRUE)\npre_columns_to_include &lt;- names(pre_sorted_proportions[pre_sorted_proportions &gt; threshold])\n\nproportion_negative_nine_post &lt;- sapply(post_missing_data, function(x) mean(x == -9))\npost_sorted_proportions &lt;- sort(proportion_negative_nine_post, decreasing = TRUE)\npost_columns_to_include &lt;- names(post_sorted_proportions[post_sorted_proportions &gt; threshold])\n\n# helper to map the question codes to full questions\nquestion_mapping &lt;- c(\n  \"V201151\" = \"Rating: Joe Biden?\",\n  \"V201152\" = \"Rating: Donald Trump?\",\n  \"V201153\" = \"Rating: Kamala Harris?\",\n  \"V201154\" = \"Rating: Mike Pence?\",\n  \"V201203\" = \"Scale Liberal-Conservative:\\nDonald Trump\",\n  \"V201204\" = \"Scale Liberal-Conservative:\\nDemocratic House Candidate\",\n  \"V201205\" = \"Scale Liberal-Conservative:\\nRepublican House Candidate\",\n  \"V201338\" = \"Stand on Abortion:\\nDemocratic Candidates\",\n  \"V201339\" = \"Stand on Abortion:\\nRepublican Candidates\",\n  \"V201409\" = \"Transgender Policy\",\n  \"V201529\" = \"Responder's Employment\",\n  \"V201576\" = \"How long has Responder lived\\nin the present community?\",\n  \"V201606\" = \"Money Invested in Stock Markets\",\n  \"V201628\" = \"Number of Guns Owned\",\n  \"V201642\" = \"Political Knowledge\\nCatch Question (Hard)\",\n  \"V201644\" = \"Political Knowledge\\nCatch Question (Easy)\"\n)\n\n\n\n\nCode\n# missing data for pre-elections: proportions\nmissing_df &lt;- data.frame(variable = names(proportion_negative_nine_pre), \n                          missing_proportion = proportion_negative_nine_pre)\nmissing_df_filtered &lt;- missing_df[missing_df$missing_proportion &gt; threshold, ]\nmissing_df_filtered$variable_label &lt;- question_mapping[missing_df_filtered$variable]\n\nggplot(missing_df_filtered, aes(x = reorder(variable_label, -missing_proportion), y = missing_proportion)) +\n  geom_bar(stat = \"identity\", fill = \"blue\") +\n  labs(title = \"Proportion of Missing Values in Pre-Election 2020 Data (Threshold &gt; 2%)\",\n       x = \"Questions\", y = \"Missing Proportion\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nThe large amount of data makes it necessary to take a very selective approach when looking at missing values. For the purpose of this particular analysis, an emphasis is put on the questions in the pre-election data where the proportion of missing values is above 2%. This threshold was chosen somewhat arbitrarily, but it helps separate the clear outliers from the many questions with missing values within the 1% range.\nNot surprisingly, the political knowledge hard catch question has the highest proportion of missing values, with more than 10% of respondents unable to answer it. While this stands out as an extreme case, the next two highest proportions (between 7.5% and 10%) are more interesting. These come from questions asking respondents to place their local Democratic and Republican House candidates on the liberal-conservative scale. Since most respondents are probably familiar with the scale, it is unlikely that a lack of understanding caused the high missing rates here. Instead, it might suggest that many respondents simply do not know their local party candidates, which could be even more common if the candidate is from the opposite party.\nThe fourth-highest proportion of missing values comes from the question about the number of guns owned, which is likely due to the sensitive nature of this topic. People may be hesitant to share such personal and identifying information.\nThe fifth-highest missing value proportion comes from the question about rating Kamala Harris, who was running for vice president at the time. In 2020, Harris seemed to have a much higher rate of unrecognizability compared to Mike Pence, Donald Trump’s running mate. This might be because Harris was newer to the national political stage, while Pence was already a well-known figure. Possibly, this could have started a domino-effect extending into the 2024 presidential elections.\n\n\nCode\n# missing data for pre-elections: heatmap\nset.seed(123)\nsampled_data &lt;- pre_missing_data[sample(nrow(pre_missing_data), 100), ]\n\ntidy_sampled_data &lt;- sampled_data |&gt;\n  rownames_to_column(\"id\") |&gt;\n  pivot_longer(cols = all_of(pre_columns_to_include)) |&gt;\n  mutate(missing = ifelse(value == -9, \"Yes\", \"No\"))\n\ntidy_sampled_data$variable_label &lt;- question_mapping[tidy_sampled_data$name]\n\nquestion_order &lt;- c(\"Rating: Joe Biden?\",\"Rating: Donald Trump?\",\"Rating: Kamala Harris?\",\"Rating: Mike Pence?\",\n                    \"Scale Liberal-Conservative:\\nDonald Trump\",\"Scale Liberal-Conservative:\\nDemocratic House Candidate\",\n                    \"Scale Liberal-Conservative:\\nRepublican House Candidate\",\"Stand on Abortion:\\nDemocratic Candidates\",\n                    \"Stand on Abortion:\\nRepublican Candidates\",\"Transgender Policy\",\"Responder's Employment\",\n                    \"How long has Responder lived\\nin the present community?\",\"Money Invested in Stock Markets\",\n                    \"Number of Guns Owned\",\"Political Knowledge\\nCatch Question (Hard)\",\"Political Knowledge\\nCatch Question (Easy)\"\n)\n\ntidy_sampled_data$variable_label &lt;- factor(\n  tidy_sampled_data$variable_label,\n  levels = question_order\n)\n\nggplot(tidy_sampled_data, aes(x = variable_label, y = id, fill = missing)) +\n  geom_tile(color = \"white\") +\n  scale_fill_viridis_d() +\n  labs(title = paste(\"Pre-Elections 2020 Data with Missing Values (100 Random Samples)\"),\n       x = \"Variables\", y = \"Sample Number\", fill = \"Missing\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCreating a meaningful heatmap to show missing value patterns for the same individuals across different questions is tricky because of the large amount of data. To make it more manageable, a (pseudo)random sample of 100 individuals is used, along with the same subset of questions as before–those with more than 2% missing values. However, this method makes it hard to tell if a sequence of missing values happens because someone decided to stop participating in the study. If that is the case, the missing values (shown in yellow) would extend to the far-right side of the heatmap, but there is no way to be sure with the chosen approach.\nSome patterns seen earlier show up here, too. For example, respondents who skipped the political knowledge hard catch question also tended to skip the questions about placing their local Democratic and Republican House candidates on the liberal-conservative scale.\nMoreover, there seem to be two common patterns when it comes to missing answers. In some cases, the respondent struggles with just one question and skips it rather than selecting “I don’t know.” In other cases, the person skips several questions, which could happen because they lack the knowledge needed to answer these or, perhaps, because they want to finish the survey quickly. While neither hypothesis can be confirmed based on this small sample, these patterns are worth exploring further. For instance, if someone skips just one question about recognizing a candidate, it might mean they truly do not know who that candidate is. On the other hand, skipping multiple questions, like rating or demographic questions, could be a way to hide the respondent’s preferences or identity. Both possibilities are worth considering, and more detailed visualizations could help confirm these trends.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  }
]